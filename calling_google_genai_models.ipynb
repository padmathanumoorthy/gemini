{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyOvliajDcqDOdFEZ84qWXsK",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/padmathanumoorthy/gemini/blob/master/calling_google_genai_models.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5OnbDHF2gx3P"
      },
      "outputs": [],
      "source": [
        "pip install -q google-generativeai\n",
        "# !pip install --upgrade -q google-generativeai"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip show google-generativeai\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-_qJUTWeg2fI",
        "outputId": "c7123cbc-2cf3-49a0-8bee-244ef6490f82"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Name: google-generativeai\n",
            "Version: 0.3.2\n",
            "Summary: Google Generative AI High level API client library and tools.\n",
            "Home-page: https://github.com/google/generative-ai-python\n",
            "Author: Google LLC\n",
            "Author-email: googleapis-packages@google.com\n",
            "License: Apache 2.0\n",
            "Location: /usr/local/lib/python3.10/dist-packages\n",
            "Requires: google-ai-generativelanguage, google-api-core, google-auth, protobuf, tqdm, typing-extensions\n",
            "Required-by: \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install -q python.dotenv"
      ],
      "metadata": {
        "id": "D_OsTEKBiptu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#if we are using jupyter notebook store the keys in the .env file and\n",
        "#access them using the python.dotenv lib\n",
        "#pip install\n",
        "#gemini_api_key = \"<key>\" store as .env select AllFiles\n",
        "#file names with .env will be hidden"
      ],
      "metadata": {
        "id": "sAATaiEVhFUV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import google.generativeai as genai\n",
        "\n",
        "import os\n",
        "from dotenv import load_dotenv, find_dotenv\n",
        "\n",
        "load_dotenv(find_dotenv(), override=True) #this will be true in the local notebooks but false here"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RhohZJRThywn",
        "outputId": "17a73357-7697-4640-b8f0-33de2597a9e6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "False"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "os.environ.get('gemini_api_key')\n",
        "#the above will store the key from the .env file"
      ],
      "metadata": {
        "id": "59GlzETjibM8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "genai.configure(api_key=userdata.get('gemini_api_key'))\n",
        "\n",
        "#listing available models\n",
        "for m in genai.list_models():\n",
        "  print (m)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "oppr8jMsjUjS",
        "outputId": "5a8dee8a-e40d-442f-896c-182af8f31c21"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model(name='models/chat-bison-001',\n",
            "      base_model_id='',\n",
            "      version='001',\n",
            "      display_name='PaLM 2 Chat (Legacy)',\n",
            "      description='A legacy text-only model optimized for chat conversations',\n",
            "      input_token_limit=4096,\n",
            "      output_token_limit=1024,\n",
            "      supported_generation_methods=['generateMessage', 'countMessageTokens'],\n",
            "      temperature=0.25,\n",
            "      top_p=0.95,\n",
            "      top_k=40)\n",
            "Model(name='models/text-bison-001',\n",
            "      base_model_id='',\n",
            "      version='001',\n",
            "      display_name='PaLM 2 (Legacy)',\n",
            "      description='A legacy model that understands text and generates text as an output',\n",
            "      input_token_limit=8196,\n",
            "      output_token_limit=1024,\n",
            "      supported_generation_methods=['generateText', 'countTextTokens', 'createTunedTextModel'],\n",
            "      temperature=0.7,\n",
            "      top_p=0.95,\n",
            "      top_k=40)\n",
            "Model(name='models/embedding-gecko-001',\n",
            "      base_model_id='',\n",
            "      version='001',\n",
            "      display_name='Embedding Gecko',\n",
            "      description='Obtain a distributed representation of a text.',\n",
            "      input_token_limit=1024,\n",
            "      output_token_limit=1,\n",
            "      supported_generation_methods=['embedText', 'countTextTokens'],\n",
            "      temperature=None,\n",
            "      top_p=None,\n",
            "      top_k=None)\n",
            "Model(name='models/gemini-1.0-pro',\n",
            "      base_model_id='',\n",
            "      version='001',\n",
            "      display_name='Gemini 1.0 Pro',\n",
            "      description='The best model for scaling across a wide range of tasks',\n",
            "      input_token_limit=30720,\n",
            "      output_token_limit=2048,\n",
            "      supported_generation_methods=['generateContent', 'countTokens'],\n",
            "      temperature=0.9,\n",
            "      top_p=1.0,\n",
            "      top_k=1)\n",
            "Model(name='models/gemini-1.0-pro-001',\n",
            "      base_model_id='',\n",
            "      version='001',\n",
            "      display_name='Gemini 1.0 Pro 001',\n",
            "      description=('The best model for scaling across a wide range of tasks. This is a stable '\n",
            "                   'model.'),\n",
            "      input_token_limit=30720,\n",
            "      output_token_limit=2048,\n",
            "      supported_generation_methods=['generateContent', 'countTokens'],\n",
            "      temperature=0.9,\n",
            "      top_p=1.0,\n",
            "      top_k=1)\n",
            "Model(name='models/gemini-1.0-pro-latest',\n",
            "      base_model_id='',\n",
            "      version='001',\n",
            "      display_name='Gemini 1.0 Pro Latest',\n",
            "      description=('The best model for scaling across a wide range of tasks. This is the latest '\n",
            "                   'model.'),\n",
            "      input_token_limit=30720,\n",
            "      output_token_limit=2048,\n",
            "      supported_generation_methods=['generateContent', 'countTokens'],\n",
            "      temperature=0.9,\n",
            "      top_p=1.0,\n",
            "      top_k=1)\n",
            "Model(name='models/gemini-1.0-pro-vision-latest',\n",
            "      base_model_id='',\n",
            "      version='001',\n",
            "      display_name='Gemini 1.0 Pro Vision',\n",
            "      description='The best image understanding model to handle a broad range of applications',\n",
            "      input_token_limit=12288,\n",
            "      output_token_limit=4096,\n",
            "      supported_generation_methods=['generateContent', 'countTokens'],\n",
            "      temperature=0.4,\n",
            "      top_p=1.0,\n",
            "      top_k=32)\n",
            "Model(name='models/gemini-pro',\n",
            "      base_model_id='',\n",
            "      version='001',\n",
            "      display_name='Gemini 1.0 Pro',\n",
            "      description='The best model for scaling across a wide range of tasks',\n",
            "      input_token_limit=30720,\n",
            "      output_token_limit=2048,\n",
            "      supported_generation_methods=['generateContent', 'countTokens'],\n",
            "      temperature=0.9,\n",
            "      top_p=1.0,\n",
            "      top_k=1)\n",
            "Model(name='models/gemini-pro-vision',\n",
            "      base_model_id='',\n",
            "      version='001',\n",
            "      display_name='Gemini 1.0 Pro Vision',\n",
            "      description='The best image understanding model to handle a broad range of applications',\n",
            "      input_token_limit=12288,\n",
            "      output_token_limit=4096,\n",
            "      supported_generation_methods=['generateContent', 'countTokens'],\n",
            "      temperature=0.4,\n",
            "      top_p=1.0,\n",
            "      top_k=32)\n",
            "Model(name='models/embedding-001',\n",
            "      base_model_id='',\n",
            "      version='001',\n",
            "      display_name='Embedding 001',\n",
            "      description='Obtain a distributed representation of a text.',\n",
            "      input_token_limit=2048,\n",
            "      output_token_limit=1,\n",
            "      supported_generation_methods=['embedContent', 'countTextTokens'],\n",
            "      temperature=None,\n",
            "      top_p=None,\n",
            "      top_k=None)\n",
            "Model(name='models/aqa',\n",
            "      base_model_id='',\n",
            "      version='001',\n",
            "      display_name='Model that performs Attributed Question Answering.',\n",
            "      description=('Model trained to return answers to questions that are grounded in provided '\n",
            "                   'sources, along with estimating answerable probability.'),\n",
            "      input_token_limit=7168,\n",
            "      output_token_limit=1024,\n",
            "      supported_generation_methods=['generateAnswer'],\n",
            "      temperature=0.2,\n",
            "      top_p=1.0,\n",
            "      top_k=40)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#listing available models with supported_generation_methods contain ['generateContent']\n",
        "for m in genai.list_models():\n",
        "  if 'generateContent' in m.supported_generation_methods:\n",
        "    print(m.name)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121
        },
        "id": "lLG-lSINjwxB",
        "outputId": "f9012a9d-6120-4689-d05a-cad1ed879d2b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "models/gemini-1.0-pro\n",
            "models/gemini-1.0-pro-001\n",
            "models/gemini-1.0-pro-latest\n",
            "models/gemini-1.0-pro-vision-latest\n",
            "models/gemini-pro\n",
            "models/gemini-pro-vision\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Gemini - Introduction\n",
        "\n",
        "Gemini is a family of highly capable multimodal models developed by Google DeepMind\n",
        "\n",
        "Multimodal model is an AI model that can process and understand information from various sources such as text, audio and videos, and code, images\n",
        "\n",
        "contrast traditional models are trained with the single type of data such as text or audio or images or videos\n",
        "\n",
        "Gemini was trained on a massive dataset from various sources such as text, audio and videos, and code, images.\n",
        "\n",
        "It has both broad capabilities across modalities and exceptional performance in each domain.\n",
        "\n",
        "training dataset is multimodal and multilingual\n",
        "\n",
        "\n",
        "- Gemini Ultra (largest) - the largest and most capable model\n",
        "- Gemini Pro - the best model for the scaling\n",
        "- Gemini Nano - 1.8B (Nano1) and 3.2B (Nano2) Parameters. The most effi.model for on-device (mobile) deployments. brings AI closer to the user\n",
        "\n",
        "usage: Texttile inputs, natural images, pdfs, etc\n",
        "\n",
        "Gemini Paper: arxiv.org/abs/2312.11805\n",
        "\n",
        "(like GPT - Generative pre-trained transformer) Gemini models build on top of tranformer technology, they are trianed to support the 32K context length, employing efficient attention mechanisms.\n",
        "\n",
        "\n",
        "Benchmarked against, factuality, Long-context, Math/Science, Summarization, Reasoning, multilinguality\n",
        "\n",
        "Developers can access Gemini pro via the Gemini API in Google AI studio in Google cloud vertex AI.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "PAGe9Scvw3DO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Google AI studio\n",
        "\n",
        "This is a browser based IDE for prototyping with gen AI models.\n",
        "\n",
        "ai.google.dev\n",
        "\n",
        "get API\n",
        "when you are done we can explore it to code\n",
        "several intervaces\n",
        "\n",
        "- freefrom prompot\n",
        "-strcutred prompt\n",
        "-chat prompt\n",
        "\n",
        "Prompt : look at the following pictures ad idenfying them\n",
        "upload the picture\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "O9fMaiqicAaB"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "KhSRDryrwzJ4"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPACSCQHKbx52YTG5fmSkVw",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/padmathanumoorthy/gemini/blob/master/Gemini_gen_parameters.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "baNxqHvH_3rV"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Gemini API Generation Parameters"
      ],
      "metadata": {
        "id": "v1nmf8CF_4yB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import google.generativeai as genai\n",
        "from google.colab import userdata\n",
        "genai.configure(api_key=userdata.get('API_KEY'))\n",
        "\n",
        "model = genai.GenerativeModel('gemini-pro')"
      ],
      "metadata": {
        "id": "ibAX1-uj_5az"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"rewrite 'jana gana mana' from india anthem to modern song\"\n",
        "response = model.generate_content(prompt)\n",
        "print(response.text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 729
        },
        "id": "3Oh8ikPXARDk",
        "outputId": "78ed56ef-39a2-4bbe-92ad-a2499721ece5"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "**Verse 1:**\n",
            "In union's realm, we stand as one,\n",
            "Bharat's children, heart and soul.\n",
            "From Kashmir's heights to Kanyakumari's shore,\n",
            "Our unity unfurls its vibrant glow.\n",
            "\n",
            "**Chorus:**\n",
            "Jana Gana Mana, our anthem grand,\n",
            "A symphony of countless voices, hand in hand.\n",
            "Long reign our motherland, India our pride,\n",
            "Where dreams take flight and freedom resides.\n",
            "\n",
            "**Verse 2:**\n",
            "In every note, a million stories unfold,\n",
            "Of struggles endured and victories told.\n",
            "The valor of our ancestors guides our way,\n",
            "Their legacy, a beacon in the darkest day.\n",
            "\n",
            "**Verse 3:**\n",
            "Through trials and triumphs, we've stood the test,\n",
            "Our spirits unyielding, our hearts blessed.\n",
            "With innovation's flame, we surge ahead,\n",
            "In science, arts, and dreams that spread.\n",
            "\n",
            "**Bridge:**\n",
            "From bustling cities to tranquil towns,\n",
            "Our diversity weaves a vibrant crown.\n",
            "Languages, customs, a rich tapestry,\n",
            "Uniting us in a symphony.\n",
            "\n",
            "**Chorus:**\n",
            "Jana Gana Mana, our anthem grand,\n",
            "A symphony of countless voices, hand in hand.\n",
            "Long reign our motherland, India our pride,\n",
            "Where dreams take flight and freedom resides.\n",
            "\n",
            "**Outro:**\n",
            "Together we rise, in unity we soar,\n",
            "Jana Gana Mana, forevermore.\n",
            "Our anthem echoes through the halls of time,\n",
            "A testament to our indomitable climb.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "generate_config = genai.types.GenerationConfig()\n",
        "generate_config"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rskHp1EGAsDK",
        "outputId": "ed85eada-e88d-48e5-d583-64dfbe20fbe7"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "GenerationConfig(candidate_count=None, stop_sequences=None, max_output_tokens=None, temperature=None, top_p=None, top_k=None)"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "help(genai.types.GenerationConfig)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OcHFdyVeBAsE",
        "outputId": "23052e1e-8d51-479e-a141-cc96dc156170"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Help on class GenerationConfig in module google.generativeai.types.generation_types:\n",
            "\n",
            "class GenerationConfig(builtins.object)\n",
            " |  GenerationConfig(candidate_count: 'int | None' = None, stop_sequences: 'Iterable[str] | None' = None, max_output_tokens: 'int | None' = None, temperature: 'float | None' = None, top_p: 'float | None' = None, top_k: 'int | None' = None) -> None\n",
            " |  \n",
            " |  A simple dataclass used to configure the generation parameters of `GenerativeModel.generate_content`.\n",
            " |  \n",
            " |  Attributes:\n",
            " |      candidate_count:\n",
            " |          Number of generated responses to return.\n",
            " |      stop_sequences:\n",
            " |          The set of character sequences (up\n",
            " |          to 5) that will stop output generation. If\n",
            " |          specified, the API will stop at the first\n",
            " |          appearance of a stop sequence. The stop sequence\n",
            " |          will not be included as part of the response.\n",
            " |      max_output_tokens:\n",
            " |          The maximum number of tokens to include in a\n",
            " |          candidate.\n",
            " |  \n",
            " |          If unset, this will default to output_token_limit specified\n",
            " |          in the model's specification.\n",
            " |      temperature:\n",
            " |          Controls the randomness of the output. Note: The\n",
            " |  \n",
            " |          default value varies by model, see the `Model.temperature`\n",
            " |          attribute of the `Model` returned the `genai.get_model`\n",
            " |          function.\n",
            " |  \n",
            " |          Values can range from [0.0,1.0], inclusive. A value closer\n",
            " |          to 1.0 will produce responses that are more varied and\n",
            " |          creative, while a value closer to 0.0 will typically result\n",
            " |          in more straightforward responses from the model.\n",
            " |      top_p:\n",
            " |          Optional. The maximum cumulative probability of tokens to\n",
            " |          consider when sampling.\n",
            " |  \n",
            " |          The model uses combined Top-k and nucleus sampling.\n",
            " |  \n",
            " |          Tokens are sorted based on their assigned probabilities so\n",
            " |          that only the most likely tokens are considered. Top-k\n",
            " |          sampling directly limits the maximum number of tokens to\n",
            " |          consider, while Nucleus sampling limits number of tokens\n",
            " |          based on the cumulative probability.\n",
            " |  \n",
            " |          Note: The default value varies by model, see the\n",
            " |          `Model.top_p` attribute of the `Model` returned the\n",
            " |          `genai.get_model` function.\n",
            " |  \n",
            " |      top_k (int):\n",
            " |          Optional. The maximum number of tokens to consider when\n",
            " |          sampling.\n",
            " |  \n",
            " |          The model uses combined Top-k and nucleus sampling.\n",
            " |  \n",
            " |          Top-k sampling considers the set of `top_k` most probable\n",
            " |          tokens. Defaults to 40.\n",
            " |  \n",
            " |          Note: The default value varies by model, see the\n",
            " |          `Model.top_k` attribute of the `Model` returned the\n",
            " |          `genai.get_model` function.\n",
            " |  \n",
            " |  Methods defined here:\n",
            " |  \n",
            " |  __eq__(self, other)\n",
            " |      Return self==value.\n",
            " |  \n",
            " |  __init__(self, candidate_count: 'int | None' = None, stop_sequences: 'Iterable[str] | None' = None, max_output_tokens: 'int | None' = None, temperature: 'float | None' = None, top_p: 'float | None' = None, top_k: 'int | None' = None) -> None\n",
            " |      Initialize self.  See help(type(self)) for accurate signature.\n",
            " |  \n",
            " |  __repr__(self)\n",
            " |      Return repr(self).\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Data descriptors defined here:\n",
            " |  \n",
            " |  __dict__\n",
            " |      dictionary for instance variables (if defined)\n",
            " |  \n",
            " |  __weakref__\n",
            " |      list of weak references to the object (if defined)\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Data and other attributes defined here:\n",
            " |  \n",
            " |  __annotations__ = {'candidate_count': 'int | None', 'max_output_tokens...\n",
            " |  \n",
            " |  __dataclass_fields__ = {'candidate_count': Field(name='candidate_count...\n",
            " |  \n",
            " |  __dataclass_params__ = _DataclassParams(init=True,repr=True,eq=True,or...\n",
            " |  \n",
            " |  __hash__ = None\n",
            " |  \n",
            " |  __match_args__ = ('candidate_count', 'stop_sequences', 'max_output_tok...\n",
            " |  \n",
            " |  candidate_count = None\n",
            " |  \n",
            " |  max_output_tokens = None\n",
            " |  \n",
            " |  stop_sequences = None\n",
            " |  \n",
            " |  temperature = None\n",
            " |  \n",
            " |  top_k = None\n",
            " |  \n",
            " |  top_p = None\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "genai.get_model('models/gemini-pro')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 208
        },
        "id": "__Nvnzo3BXEL",
        "outputId": "7adce543-73c9-4df5-94ef-99b04a5232a2"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Model(name='models/gemini-pro',\n",
              "      base_model_id='',\n",
              "      version='001',\n",
              "      display_name='Gemini 1.0 Pro',\n",
              "      description='The best model for scaling across a wide range of tasks',\n",
              "      input_token_limit=30720,\n",
              "      output_token_limit=2048,\n",
              "      supported_generation_methods=['generateContent', 'countTokens'],\n",
              "      temperature=0.9,\n",
              "      top_p=1.0,\n",
              "      top_k=1)"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "genai.get_model('models/gemini-pro-vision')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 208
        },
        "id": "fgxq1xNKBq9h",
        "outputId": "a8b8a929-60e6-4ae6-f2f5-9303b556124a"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Model(name='models/gemini-pro-vision',\n",
              "      base_model_id='',\n",
              "      version='001',\n",
              "      display_name='Gemini 1.0 Pro Vision',\n",
              "      description='The best image understanding model to handle a broad range of applications',\n",
              "      input_token_limit=12288,\n",
              "      output_token_limit=4096,\n",
              "      supported_generation_methods=['generateContent', 'countTokens'],\n",
              "      temperature=0.4,\n",
              "      top_p=1.0,\n",
              "      top_k=32)"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "generate_config = genai.types.GenerationConfig()\n",
        "generate_config\n",
        "#override the default parameters\n",
        "model = genai.GenerativeModel ('gemini-pro', generation_config= generate_config)\n",
        "response = model.generate_content(prompt)\n",
        "print(response.text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 833
        },
        "id": "RyJ0Pq38B539",
        "outputId": "407f230d-fa60-46c8-bce5-3190d35eb26e"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "**Verse 1**\n",
            "Yo, listen up, let's rise and groove\n",
            "To the anthem that makes our hearts move\n",
            "Jana Gana Mana, our motherland's soul\n",
            "United we stand, stronger than a toll\n",
            "\n",
            "**Chorus**\n",
            "Hey, hey, let's sing it loud\n",
            "Jana Gana Mana, our nation's proud\n",
            "From mountains high to roaring seas\n",
            "Our unity echoes, for all eternity\n",
            "\n",
            "**Verse 2**\n",
            "In diversity, our strength resides\n",
            "Languages and cultures, a kaleidoscope of tides\n",
            "Guitars strum, drums beat, melodies soar\n",
            "We're a symphony, forevermore\n",
            "\n",
            "**Chorus**\n",
            "Hey, hey, let's sing it loud\n",
            "Jana Gana Mana, our nation's proud\n",
            "From mountains high to roaring seas\n",
            "Our unity echoes, for all eternity\n",
            "\n",
            "**Bridge**\n",
            "Through trials and triumphs, we've come this far\n",
            "Resilience within us, shining like a star\n",
            "Let's build a future, where dreams take flight\n",
            "United in harmony, our nation's light\n",
            "\n",
            "**Verse 3**\n",
            "In this digital age, where connections meet\n",
            "Our anthem connects, making our hearts beat\n",
            "From Delhi's streets to Mumbai's bay\n",
            "Jana Gana Mana, our guiding ray\n",
            "\n",
            "**Chorus**\n",
            "Hey, hey, let's sing it loud\n",
            "Jana Gana Mana, our nation's proud\n",
            "From mountains high to roaring seas\n",
            "Our unity echoes, for all eternity\n",
            "\n",
            "**Outro**\n",
            "Oh, Jana Gana Mana, our guiding guide\n",
            "We'll carry your spirit, with unwavering pride\n",
            "United we stand, for generations to come\n",
            "Our anthem, our heartbeat, forever one\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "generate_config = genai.types.GenerationConfig(\n",
        "   candidate_count=1,\n",
        "   stop_sequences=[','],\n",
        "   max_output_tokens=32000,\n",
        "   temperature=0.9,\n",
        "   top_p=1,\n",
        "   top_k=1\n",
        ")\n",
        "#override the default parameters\n",
        "model = genai.GenerativeModel ('gemini-pro', generation_config= generate_config)\n",
        "response = model.generate_content(prompt)\n",
        "print(response.text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "id": "YU4_3_6lCNGw",
        "outputId": "47905e28-9058-4745-ea98-af8d058cd941"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "**Verse 1:**\n",
            "Yo\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Parameter explained\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Sz5wwxpkDY5t"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Help on class GenerationConfig in module google.generativeai.types.generation_types:\n",
        "\n",
        "class GenerationConfig(builtins.object)\n",
        " |  GenerationConfig(candidate_count: 'int | None' = None, stop_sequences: 'Iterable[str] | None' = None, max_output_tokens: 'int | None' = None, temperature: 'float | None' = None, top_p: 'float | None' = None, top_k: 'int | None' = None) -> None\n",
        " |  \n",
        " |  A simple dataclass used to configure the generation parameters of `GenerativeModel.generate_content`.\n",
        " |  \n",
        " |  Attributes:\n",
        " |      candidate_count:\n",
        " |          Number of generated responses to return.\n",
        " |      stop_sequences:\n",
        " |          The set of character sequences (up\n",
        " |          to 5) that will stop output generation. If\n",
        " |          specified, the API will stop at the first\n",
        " |          appearance of a stop sequence. The stop sequence\n",
        " |          will not be included as part of the response.\n",
        " |      max_output_tokens:\n",
        " |          The maximum number of tokens to include in a\n",
        " |          candidate.\n",
        " |  \n",
        " |          If unset, this will default to output_token_limit specified\n",
        " |          in the model's specification.\n",
        " |      temperature:\n",
        " |          Controls the randomness of the output. Note: The\n",
        " |  \n",
        " |          default value varies by model, see the `Model.temperature`\n",
        " |          attribute of the `Model` returned the `genai.get_model`\n",
        " |          function.\n",
        " |  \n",
        " |          Values can range from [0.0,1.0], inclusive. A value closer\n",
        " |          to 1.0 will produce responses that are more varied and\n",
        " |          creative, while a value closer to 0.0 will typically result\n",
        " |          in more straightforward responses from the model.\n",
        " |      top_p:\n",
        " |          Optional. The maximum cumulative probability of tokens to\n",
        " |          consider when sampling.\n",
        " |  \n",
        " |          The model uses combined Top-k and nucleus sampling.\n",
        " |  \n",
        " |          Tokens are sorted based on their assigned probabilities so\n",
        " |          that only the most likely tokens are considered. Top-k\n",
        " |          sampling directly limits the maximum number of tokens to\n",
        " |          consider, while Nucleus sampling limits number of tokens\n",
        " |          based on the cumulative probability.\n",
        " |  \n",
        " |          Note: The default value varies by model, see the\n",
        " |          `Model.top_p` attribute of the `Model` returned the\n",
        " |          `genai.get_model` function.\n",
        " |  \n",
        " |      top_k (int):\n",
        " |          Optional. The maximum number of tokens to consider when\n",
        " |          sampling.\n",
        " |  \n",
        " |          The model uses combined Top-k and nucleus sampling.\n",
        " |  \n",
        " |          Top-k sampling considers the set of `top_k` most probable\n",
        " |          tokens. Defaults to 40.\n",
        " |  \n",
        " |          Note: The default value varies by model, see the\n",
        " |          `Model.top_k` attribute of the `Model` returned the\n",
        " |          `genai.get_model` function.\n",
        " |  \n",
        " |  Methods defined here:\n",
        " |  \n",
        " |  __eq__(self, other)\n",
        " |      Return self==value.\n",
        " |  \n",
        " |  __init__(self, candidate_count: 'int | None' = None, stop_sequences: 'Iterable[str] | None' = None, max_output_tokens: 'int | None' = None, temperature: 'float | None' = None, top_p: 'float | None' = None, top_k: 'int | None' = None) -> None\n",
        " |      Initialize self.  See help(type(self)) for accurate signature.\n",
        " |  \n",
        " |  __repr__(self)\n",
        " |      Return repr(self).\n",
        " |  \n",
        " |  ----------------------------------------------------------------------\n",
        " |  Data descriptors defined here:\n",
        " |  \n",
        " |  __dict__\n",
        " |      dictionary for instance variables (if defined)\n",
        " |  \n",
        " |  __weakref__\n",
        " |      list of weak references to the object (if defined)\n",
        " |  \n",
        " |  ----------------------------------------------------------------------\n",
        " |  Data and other attributes defined here:\n",
        " |  \n",
        " |  __annotations__ = {'candidate_count': 'int | None', 'max_output_tokens...\n",
        " |  \n",
        " |  __dataclass_fields__ = {'candidate_count': Field(name='candidate_count...\n",
        " |  \n",
        " |  __dataclass_params__ = _DataclassParams(init=True,repr=True,eq=True,or...\n",
        " |  \n",
        " |  __hash__ = None\n",
        " |  \n",
        " |  __match_args__ = ('candidate_count', 'stop_sequences', 'max_output_tok...\n",
        " |  \n",
        " |  candidate_count = None\n",
        " |  \n",
        " |  max_output_tokens = None\n",
        " |  \n",
        " |  stop_sequences = None\n",
        " |  \n",
        " |  temperature = None\n",
        " |  \n",
        " |  top_k = None\n",
        " |  \n",
        " |  top_p = None"
      ],
      "metadata": {
        "id": "SXJBuCw4Ed7y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "generate_config = genai.types.GenerationConfig(\n",
        "   candidate_count=1,\n",
        "   #stop_sequences=[';'],  #run with an without ;\n",
        "   max_output_tokens=32000,\n",
        "   temperature=0.9,\n",
        "   top_p=1,\n",
        "   top_k=1\n",
        ")\n",
        "#override the default parameters\n",
        "model = genai.GenerativeModel ('gemini-pro', generation_config= generate_config)\n",
        "prompt = 'give me an example of an SQL SELECT statement'\n",
        "response = model.generate_content(prompt)\n",
        "print(response.text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121
        },
        "id": "PwTVBWUtEeNk",
        "outputId": "eb5f13d1-3052-4cf7-a179-614cd942af61"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "```sql\n",
            "SELECT customer_id, customer_name, email, phone\n",
            "FROM customers\n",
            "WHERE city = 'London'\n",
            "AND order_count > 5;\n",
            "```\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "generate_config = genai.types.GenerationConfig(\n",
        "   candidate_count=1,\n",
        "   stop_sequences=[';'],  #run with an without ;\n",
        "   max_output_tokens=32000,\n",
        "   temperature=0.9,\n",
        "   top_p=1,\n",
        "   top_k=1\n",
        ")\n",
        "#override the default parameters\n",
        "model = genai.GenerativeModel ('gemini-pro', generation_config= generate_config)\n",
        "prompt = 'give me an example of an SQL SELECT statement'\n",
        "response = model.generate_content(prompt)\n",
        "print(response.text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 86
        },
        "id": "m-xfAhCoFKz2",
        "outputId": "6f9d67fa-514f-48e2-b021-aaee65fb2a0a"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "```sql\n",
            "SELECT <columns>\n",
            "FROM <table_name>\n",
            "WHERE <condition>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "generate_config = genai.types.GenerationConfig(\n",
        "   candidate_count=1,\n",
        "   stop_sequences=None,  #run with an without ;\n",
        "   max_output_tokens=32000,\n",
        "   temperature=1,  #run with 0.1 and 0.0, 0.9\n",
        "   top_p=1,\n",
        "   top_k=1\n",
        ")\n",
        "#override the default parameters\n",
        "model = genai.GenerativeModel ('gemini-pro', generation_config= generate_config)\n",
        "prompt = 'what is the meaning of life'\n",
        "response = model.generate_content(prompt)\n",
        "print(response.text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 367
        },
        "id": "8CqL-FJ1FTBU",
        "outputId": "be5a6b5e-145c-402b-9d24-b5a9da0df2da"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The meaning of life is a profoundly complex and intensely personal question that has preoccupied humans throughout history. There is no single definitive answer that applies universally, as the meaning of life can vary significantly depending on individual perspectives, beliefs, and values. However, here are some common themes and potential meanings of life:\n",
            "\n",
            "1. **Purpose and Fulfillment:** Many people find meaning in life through a sense of purpose or calling. Identifying something that drives you, makes you feel accomplished, and contributes to the greater good can provide a profound sense of meaning and fulfillment.\n",
            "\n",
            "2. **Relationships and Connections:** Human relationships are a vital part of life for many people. Establishing strong and meaningful connections with family, friends, and loved ones can bring immense joy, support, and purpose.\n",
            "\n",
            "3. **Contribution to Society:** Some people find meaning in life through contributing positively to their communities, the world around them, or future generations. Acts of kindness, volunteering, and social or environmental activism can create a sense of purpose and make a lasting impact.\n",
            "\n",
            "4. **Growth and Self-Improvement:** Continuous growth and self-improvement can provide a sense of meaning and fulfillment. Striving to become better versions of ourselves, both intellectually and emotionally, can lead to a more meaningful and rewarding life.\n",
            "\n",
            "5. **Experiences and Memories:** Life is a collection of experiences, both positive and negative. Appreciating the present moment and creating lasting memories with loved ones can create a sense of meaning and bring joy.\n",
            "\n",
            "6. **Legacy and Impact:** Some people find meaning in life by leaving a legacy or making a lasting impact on the world. Creating works of art, writing books, or contributing to scientific discoveries can provide a sense of purpose and immortality.\n",
            "\n",
            "7. **Spiritual or Religious Beliefs:** For many people, spiritual or religious beliefs provide a framework for understanding the meaning of life. Faith, prayer, and connection to a higher power can bring comfort, guidance, and a sense of purpose.\n",
            "\n",
            "8. **Human Condition and Mortality:** Confronting the human condition and the inevitability of mortality can also lead to a deeper appreciation for life. Recognizing our own finitude can inspire us to make the most of our time and live life with purpose.\n",
            "\n",
            "It's important to note that the meaning of life is not something that is fixed or permanent. It can evolve and change as we grow and experience different stages of life. The key is to find what brings you a sense of purpose, fulfillment, and joy, and to live a life in accordance with that meaning.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "XRruUTibFemb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Top_k: controls the maximum number of tokens to consider when generating the text\n",
        "\n",
        "topk = 1 pro 32 p vision (pro vision is higher in creativity)\n",
        "\n",
        "tokens are sorted based on their assigned probabliities -\n",
        "\n",
        "top_k decides the max number of candidate tokenas to be considered\n",
        "\n",
        "Top_p: aka nucleous sampling - parameter controls the max cumulative propability of tokens when sampling and filtered based on top_k\n",
        "\n",
        "\n",
        "topp = 1 for both (pro and pro-vision)\n",
        "\n",
        "\n",
        "model using p and k when sampling\n",
        "\n",
        "if A B C are having the prob of 0.3,0.2,0.1 and top_p = 0.5\n",
        "then token A and B are selected ~0.5 total and\n",
        "token C is excluded\n",
        "\n",
        "Very specific have less top_k but if you need the creative then higher value top_k\n",
        "\n",
        "\n",
        "\n",
        "# Thumb: Top_k samples the most probable tokens while top-p samples a subset of the most probable tokens on their cumulative probablity"
      ],
      "metadata": {
        "id": "0NYzX139Gtfs"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "UzXl3n_1Ih4p"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}